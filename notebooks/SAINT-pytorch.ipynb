{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/arshadshk/SAINT-pytorch/blob/main/saint.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feed_Forward_block(nn.Module):\n",
    "    \"\"\"\n",
    "    out =  Relu( M_out*w1 + b1) *w2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_ff):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=dim_ff , out_features=dim_ff)\n",
    "        self.layer2 = nn.Linear(in_features=dim_ff , out_features=dim_ff)\n",
    "\n",
    "    def forward(self,ffn_in):\n",
    "        return  self.layer2(   F.relu( self.layer1(ffn_in) )   )\n",
    "        \n",
    "\n",
    "class Encoder_block(nn.Module):\n",
    "    \"\"\"\n",
    "    M = SkipConct(Multihead(LayerNorm(Qin;Kin;Vin)))\n",
    "    O = SkipConct(FFN(LayerNorm(M)))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self , dim_model, heads_en, total_ex ,total_cat, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embd_ex =   nn.Embedding( total_ex , embedding_dim = dim_model )                   # embedings  q,k,v = E = exercise ID embedding, category embedding, and positionembedding.\n",
    "        self.embd_cat =  nn.Embedding( total_cat, embedding_dim = dim_model )\n",
    "        self.embd_pos   = nn.Embedding(  seq_len , embedding_dim = dim_model )                  #positional embedding\n",
    "\n",
    "        self.multi_en = nn.MultiheadAttention( embed_dim= dim_model, num_heads= heads_en,  )     # multihead attention    ## todo add dropout, LayerNORM\n",
    "        self.ffn_en = Feed_Forward_block( dim_model )                                            # feedforward block     ## todo dropout, LayerNorm\n",
    "        self.layer_norm1 = nn.LayerNorm( dim_model )\n",
    "        self.layer_norm2 = nn.LayerNorm( dim_model )\n",
    "\n",
    "\n",
    "    def forward(self, in_ex, in_cat, first_block=True):\n",
    "\n",
    "        ## todo create a positional encoding ( two options numeric, sine)\n",
    "        if first_block:\n",
    "            in_ex = self.embd_ex( in_ex )\n",
    "            in_cat = self.embd_cat( in_cat )\n",
    "            #in_pos = self.embd_pos( in_pos )\n",
    "            #combining the embedings\n",
    "            out = in_ex + in_cat #+ in_pos                      # (b,n,d)\n",
    "        else:\n",
    "            out = in_ex\n",
    "        \n",
    "        in_pos = get_pos(self.seq_len)\n",
    "        in_pos = self.embd_pos( in_pos )\n",
    "        out = out + in_pos                                      # Applying positional embedding\n",
    "\n",
    "        out = out.permute(1,0,2)                                # (n,b,d)  # print('pre multi', out.shape )\n",
    "        \n",
    "        #Multihead attention                            \n",
    "        n,_,_ = out.shape\n",
    "        out = self.layer_norm1( out )                           # Layer norm\n",
    "        skip_out = out \n",
    "        out, attn_wt = self.multi_en( out , out , out ,\n",
    "                                attn_mask=get_mask(seq_len=n))  # attention mask upper triangular\n",
    "        out = out + skip_out                                    # skip connection\n",
    "\n",
    "        #feed forward\n",
    "        out = out.permute(1,0,2)                                # (b,n,d)\n",
    "        out = self.layer_norm2( out )                           # Layer norm \n",
    "        skip_out = out\n",
    "        out = self.ffn_en( out )\n",
    "        out = out + skip_out                                    # skip connection\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder_block(nn.Module):\n",
    "    \"\"\"\n",
    "    M1 = SkipConct(Multihead(LayerNorm(Qin;Kin;Vin)))\n",
    "    M2 = SkipConct(Multihead(LayerNorm(M1;O;O)))\n",
    "    L = SkipConct(FFN(LayerNorm(M2)))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,dim_model ,total_in, heads_de,seq_len  ):\n",
    "        super().__init__()\n",
    "        self.seq_len    = seq_len\n",
    "        self.embd_in    = nn.Embedding(  total_in , embedding_dim = dim_model )                  #interaction embedding\n",
    "        self.embd_pos   = nn.Embedding(  seq_len , embedding_dim = dim_model )                  #positional embedding\n",
    "        self.multi_de1  = nn.MultiheadAttention( embed_dim= dim_model, num_heads= heads_de  )  # M1 multihead for interaction embedding as q k v\n",
    "        self.multi_de2  = nn.MultiheadAttention( embed_dim= dim_model, num_heads= heads_de  )  # M2 multihead for M1 out, encoder out, encoder out as q k v\n",
    "        self.ffn_en     = Feed_Forward_block( dim_model )                                         # feed forward layer\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm( dim_model )\n",
    "        self.layer_norm2 = nn.LayerNorm( dim_model )\n",
    "        self.layer_norm3 = nn.LayerNorm( dim_model )\n",
    "\n",
    "\n",
    "    def forward(self, in_in, en_out,first_block=True):\n",
    "\n",
    "         ## todo create a positional encoding ( two options numeric, sine)\n",
    "        if first_block:\n",
    "            in_in = self.embd_in( in_in )\n",
    "\n",
    "            #combining the embedings\n",
    "            out = in_in #+ in_cat #+ in_pos                         # (b,n,d)\n",
    "        else:\n",
    "            out = in_in\n",
    "\n",
    "        in_pos = get_pos(self.seq_len)\n",
    "        in_pos = self.embd_pos( in_pos )\n",
    "        out = out + in_pos                                          # Applying positional embedding\n",
    "\n",
    "        out = out.permute(1,0,2)                                    # (n,b,d)# print('pre multi', out.shape )\n",
    "        n,_,_ = out.shape\n",
    "\n",
    "        #Multihead attention M1                                     ## todo verify if E to passed as q,k,v\n",
    "        out = self.layer_norm1( out )\n",
    "        skip_out = out\n",
    "        out, attn_wt = self.multi_de1( out , out , out, \n",
    "                                     attn_mask=get_mask(seq_len=n)) # attention mask upper triangular\n",
    "        out = skip_out + out                                        # skip connection\n",
    "\n",
    "        #Multihead attention M2                                     ## todo verify if E to passed as q,k,v\n",
    "        en_out = en_out.permute(1,0,2)                              # (b,n,d)-->(n,b,d)\n",
    "        en_out = self.layer_norm2( en_out )\n",
    "        skip_out = out\n",
    "        out, attn_wt = self.multi_de2( out , en_out , en_out,\n",
    "                                    attn_mask=get_mask(seq_len=n))  # attention mask upper triangular\n",
    "        out = out + skip_out\n",
    "\n",
    "        #feed forward\n",
    "        out = out.permute(1,0,2)                                    # (b,n,d)\n",
    "        out = self.layer_norm3( out )                               # Layer norm \n",
    "        skip_out = out\n",
    "        out = self.ffn_en( out )                                    \n",
    "        out = out + skip_out                                        # skip connection\n",
    "\n",
    "        return out\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def get_mask(seq_len):\n",
    "    ##todo add this to device\n",
    "    mask = torch.from_numpy( np.triu(np.ones((seq_len ,seq_len)), k=1).astype('bool'))\n",
    "    return mask\n",
    "\n",
    "def get_pos(seq_len):\n",
    "    # use sine positional embeddinds\n",
    "    return torch.arange( seq_len ).unsqueeze(0) \n",
    "\n",
    "class saint(nn.Module):\n",
    "    def __init__(self,dim_model,num_en, num_de ,heads_en, total_ex ,total_cat,total_in,heads_de,seq_len ):\n",
    "        super().__init__( )\n",
    "\n",
    "        self.num_en = num_en\n",
    "        self.num_de = num_de\n",
    "\n",
    "        self.encoder = get_clones( Encoder_block(dim_model, heads_en , total_ex ,total_cat,seq_len) , num_en)\n",
    "        self.decoder = get_clones( Decoder_block(dim_model ,total_in, heads_de,seq_len)             , num_de)\n",
    "\n",
    "        self.out = nn.Linear(in_features= dim_model , out_features=1)\n",
    "    \n",
    "    def forward(self,in_ex, in_cat,  in_in ):\n",
    "        \n",
    "        ## pass through each of the encoder blocks in sequence\n",
    "        first_block = True\n",
    "        for x in range(self.num_en):\n",
    "            if x>=1:\n",
    "                first_block = False\n",
    "            in_ex = self.encoder[x]( in_ex, in_cat ,first_block=first_block)\n",
    "            in_cat = in_ex                                  # passing same output as q,k,v to next encoder block\n",
    "\n",
    "        \n",
    "        ## pass through each decoder blocks in sequence\n",
    "        first_block = True\n",
    "        for x in range(self.num_de):\n",
    "            if x>=1:\n",
    "                first_block = False\n",
    "            in_in = self.decoder[x]( in_in , en_out= in_ex, first_block=first_block )\n",
    "\n",
    "        ## Output layer\n",
    "#         in_in = torch.sigmoid( self.out( in_in ) )\n",
    "        in_in = self.out(in_in)\n",
    "        print(in_in)\n",
    "        return in_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_data(bs, seq_len , total_ex, total_cat, total_in = 2):\n",
    "    ex = torch.randint( 0 , total_ex ,(bs , seq_len) )\n",
    "    cat = torch.randint( 0 , total_cat ,(bs , seq_len) )\n",
    "    de = torch.randint( 0 , total_in ,(bs , seq_len) )\n",
    "    return ex,cat, de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## forward prop on dummy data\n",
    "\n",
    "seq_len = 100\n",
    "total_ex = 1200\n",
    "total_cat = 234\n",
    "total_in = 2\n",
    "\n",
    "\n",
    "in_ex, in_cat, in_de = random_data(64, seq_len , total_ex, total_cat, total_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7084],\n",
      "         [ 0.0607],\n",
      "         [-0.4425],\n",
      "         ...,\n",
      "         [ 0.7674],\n",
      "         [ 0.4728],\n",
      "         [ 0.7629]],\n",
      "\n",
      "        [[-0.6813],\n",
      "         [ 0.0857],\n",
      "         [-0.4446],\n",
      "         ...,\n",
      "         [ 0.8193],\n",
      "         [ 0.5598],\n",
      "         [ 0.8465]],\n",
      "\n",
      "        [[-0.7208],\n",
      "         [ 0.0460],\n",
      "         [-0.4431],\n",
      "         ...,\n",
      "         [ 0.7732],\n",
      "         [ 0.5399],\n",
      "         [ 0.8179]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7625],\n",
      "         [ 0.0450],\n",
      "         [-0.4625],\n",
      "         ...,\n",
      "         [ 0.7561],\n",
      "         [ 0.4553],\n",
      "         [ 0.7349]],\n",
      "\n",
      "        [[-0.7617],\n",
      "         [ 0.0298],\n",
      "         [-0.4632],\n",
      "         ...,\n",
      "         [ 0.7574],\n",
      "         [ 0.5421],\n",
      "         [ 0.8279]],\n",
      "\n",
      "        [[-0.6977],\n",
      "         [ 0.1117],\n",
      "         [-0.4241],\n",
      "         ...,\n",
      "         [ 0.7786],\n",
      "         [ 0.5000],\n",
      "         [ 0.8431]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([64, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "model = saint(dim_model=128,   # embeddingとかのdim\n",
    "            num_en=6,   # encoder_layerの数\n",
    "            num_de=6,   # decoder_layerの数\n",
    "            heads_en=8,\n",
    "            heads_de=8,\n",
    "            total_ex=total_ex,\n",
    "            total_cat=total_cat,\n",
    "            total_in=total_in,\n",
    "            seq_len=seq_len\n",
    "            )\n",
    "\n",
    "outs = model(in_ex, in_cat, in_de)\n",
    "\n",
    "print(outs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7084,  0.0607, -0.4425,  ...,  0.7674,  0.4728,  0.7629],\n",
       "        [-0.6813,  0.0857, -0.4446,  ...,  0.8193,  0.5598,  0.8465],\n",
       "        [-0.7208,  0.0460, -0.4431,  ...,  0.7732,  0.5399,  0.8179],\n",
       "        ...,\n",
       "        [-0.7625,  0.0450, -0.4625,  ...,  0.7561,  0.4553,  0.7349],\n",
       "        [-0.7617,  0.0298, -0.4632,  ...,  0.7574,  0.5421,  0.8279],\n",
       "        [-0.6977,  0.1117, -0.4241,  ...,  0.7786,  0.5000,  0.8431]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 1,  ..., 1, 1, 0],\n",
       "        ...,\n",
       "        [0, 1, 1,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 1, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_de"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "miniconda3-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
