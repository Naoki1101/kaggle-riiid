{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformer_020_4\n",
    "\n",
    "- saintv6_2\n",
    "    - saint_v6からtask_container_id, prior_had_expを除いたもの\n",
    "- user_step_ids (step=150)\n",
    "- batch_zise: 512\n",
    "\n",
    "## files\n",
    "- ../data/input/train.csv\n",
    "- ../data/input/questions.csv\n",
    "- ../data/team/train_folds_vlatest_ALL_2p5M_v2_20201209.feather\n",
    "<!-- - ../exp/000_tran/compe.yml -->\n",
    "- ../data/team/transformer_020_4.yml\n",
    "- ../data/team/seq10/row_{}.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from torch.autograd import detect_anomaly\n",
    "import time\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import datetime\n",
    "from sklearn import metrics\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append('../src')\n",
    "from utils import (DataHandler, Timer, seed_everything)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    return metrics.roc_auc_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = '../data/input'\n",
    "FOLD_DIR = '../data/team'\n",
    "VALID_SEQ_DIR = '../data/team/seq10'\n",
    "# SAVE_DIR = '../save'\n",
    "\n",
    "FOLD_NAME = 'vlatest_ALL_2p5M'\n",
    "RANDOM_STATE = 20201209\n",
    "\n",
    "DTYPE = {\n",
    "    'row_id': 'int64',\n",
    "    'timestamp': 'int64',\n",
    "    'user_id': 'int32',\n",
    "    'content_id': 'int16',\n",
    "    'content_type_id': 'int8',\n",
    "    'task_container_id': 'int16',\n",
    "    'user_answer': 'int8',\n",
    "    'answered_correctly': 'int8',\n",
    "    'prior_question_elapsed_time': 'float32',\n",
    "    'prior_question_had_explanation': 'boolean'\n",
    "}\n",
    "\n",
    "TARGET_COLS = ['answered_correctly']\n",
    "\n",
    "model_name = 'transformer_020_4'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "run_id = f'{model_name}_{now:%Y%m%d%H%M%S}'\n",
    "EXP_NAME = f'{FOLD_NAME}__Tran'\n",
    "\n",
    "dh = DataHandler()\n",
    "cfg = dh.load('../configs/common/compe.yml')\n",
    "cfg.update(dh.load(f'../data/team/{model_name}.yml'))\n",
    "\n",
    "# cfg.data.train.params.step_len = 75\n",
    "# cfg.data.train.params.max_seq = 51\n",
    "# cfg.model.params.seq_len = 51\n",
    "# cfg.data.valid.params.max_seq = 51\n",
    "# cfg.data.test.params.max_seq = 51\n",
    "# cfg.data.train.loader.batch_size=1024\n",
    "\n",
    "# if not os.path.exists(f'{SAVE_DIR}/{EXP_NAME}_{run_id}/'):\n",
    "#     os.mkdir(f'{SAVE_DIR}/{EXP_NAME}_{run_id}/')\n",
    "#     os.mkdir(f'{SAVE_DIR}/{EXP_NAME}_{run_id}/seq_model')\n",
    "#     os.mkdir(f'{SAVE_DIR}/{EXP_NAME}_{run_id}/seq_model_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Timer()\n",
    "seed_everything(cfg.common.seed)\n",
    "\n",
    "STEP_LENGTH = cfg.data.train.params.step_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_df = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype=DTYPE, nrows=10**6)\n",
    "else:\n",
    "    train_df = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype=DTYPE)\n",
    "    \n",
    "###\n",
    "te_content = pd.read_feather('../features/te_content_id_by_answered_correctly_train.feather')\n",
    "\n",
    "if debug:\n",
    "    te_content  = te_content.iloc[:10**6]\n",
    "\n",
    "train_df['te_content_id_by_answered_correctly'] = te_content['te_content_id_by_answered_correctly'].values\n",
    "###\n",
    "\n",
    "folds = pd.read_feather(f'{FOLD_DIR}/train_folds_{FOLD_NAME}_v2_{RANDOM_STATE}.feather')\n",
    "valid_idx = folds[folds.val == 1]['index'].values\n",
    "if debug:\n",
    "    valid_idx = valid_idx[np.where(valid_idx < len(train_df))]\n",
    "\n",
    "fold_df = pd.DataFrame(index=range(len(train_df)))\n",
    "fold_df['fold_0'] = 0\n",
    "fold_df.loc[valid_idx, 'fold_0'] += 1\n",
    "\n",
    "drop_idx = train_df[train_df.content_type_id != 0].index\n",
    "train_df = train_df.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "fold_df = fold_df.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "\n",
    "def make_content_map_dict():\n",
    "    questions_df = pd.read_csv(f'{INPUT_DIR}/questions.csv')\n",
    "    q2p = dict(questions_df[['question_id', 'part']].values)\n",
    "    q2p = np.array(list(q2p.values()))   # ここを追加\n",
    "\n",
    "    questions_df['tags'] = questions_df['tags'].fillna(0)\n",
    "    questions_df['tag_list'] = questions_df['tags'].apply(lambda tags: [int(tag) for tag in str(tags).split(' ')])\n",
    "    questions_df['tag_list'] = questions_df['tag_list'].apply(lambda x: [0] * (6 - len(x)) + x)\n",
    "    q2tg = dict(questions_df[['question_id', 'tag_list']].values)\n",
    "    q2tg = np.array(list(q2tg.values()))   # ここを追加\n",
    "\n",
    "    te_dict = dh.load('../data/processed/te_content_id_by_answered_correctly.pkl')\n",
    "    te_df = pd.DataFrame.from_dict(te_dict).sort_index().iloc[:13523]\n",
    "    q2te = np.mean(te_df.values, axis=1)\n",
    "    \n",
    "    tsne0_dict = dh.load('../data/processed/tsne_encoder_0.pkl')\n",
    "    q2ts0 = np.array(list(tsne0_dict.values()))[:13523]\n",
    "    tsne1_dict = dh.load('../data/processed/tsne_encoder_1.pkl')\n",
    "    q2ts1 = np.array(list(tsne1_dict.values()))[:13523]\n",
    "\n",
    "    return q2p, q2tg, q2te, q2ts0, q2ts1\n",
    "\n",
    "q2p, q2tg, q2te, q2ts0, q2ts1 = make_content_map_dict()\n",
    "\n",
    "target_df = train_df[TARGET_COLS[0]]\n",
    "n_splits = len(fold_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# SAINT v6_2\n",
    "# mod based on CustomTrainDataset9\n",
    "\n",
    "class CustomTrainDataset7_2_(Dataset):\n",
    "    def __init__(self, samples, df, q2p, q2tg, q2te, cfg=None):\n",
    "        super(CustomTrainDataset7_2_, self).__init__()\n",
    "        self.max_seq = cfg.params.max_seq\n",
    "        self.n_content = cfg.params.n_skill\n",
    "        self.n_tag = cfg.params.total_tg\n",
    "        self.step_length = STEP_LENGTH\n",
    "        self.seq_randomness = cfg.params.seq_randomness\n",
    "        self.samples = samples\n",
    "        self.q2p = q2p\n",
    "        self.q2tg = q2tg\n",
    "        self.q2te = q2te\n",
    "\n",
    "        user_ids = []\n",
    "        for user_id in samples.index:\n",
    "            q = samples[user_id][0]\n",
    "            if len(q) < 2:\n",
    "                continue\n",
    "            user_ids.append(user_id)\n",
    "        self.user_step_ids = df[df['user_id'].isin(user_ids)]['user_step_id'].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_step_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        user_step_id = self.user_step_ids[index]\n",
    "        user_id, step_id = list(map(int, user_step_id.split('__')))\n",
    "        \n",
    "        q_, qa_, qt_, qe_, qte_ = self.samples[user_id]\n",
    "\n",
    "        step_start, step_end = step_id * self.step_length, (step_id + 1) * self.step_length\n",
    "        seq_len = len(q_[step_start: step_end])\n",
    "        if step_id > 0 and seq_len < self.step_length:\n",
    "            step_start = (step_id - 1) * self.step_length\n",
    "        elif step_id == 0 and seq_len < self.step_length:\n",
    "            step_end = random.randint(2, seq_len)\n",
    "\n",
    "        q_ = q_[step_start: step_end]\n",
    "        qa_ = qa_[step_start: step_end]\n",
    "        qt_ = qt_[step_start: step_end]\n",
    "        qe_ = qe_[step_start: step_end]\n",
    "        qte_ = qte_[step_start: step_end]\n",
    "\n",
    "        qt_ = qt_ / 60_000.   # ms -> m\n",
    "        qe_ = qe_ / 1_000.   # ms -> s\n",
    "        seq_len = len(q_)\n",
    "\n",
    "        q = np.zeros(self.max_seq, dtype=int)\n",
    "        qa = np.zeros(self.max_seq, dtype=int)\n",
    "        qt = np.zeros(self.max_seq, dtype=int)\n",
    "        qe = np.zeros(self.max_seq, dtype=int)\n",
    "        qte = np.zeros(self.max_seq, dtype=float)\n",
    "        qtg = np.zeros((self.max_seq - 1, 6), dtype=int) + self.n_tag\n",
    "        if seq_len >= self.max_seq:\n",
    "            start = random.randint(0, (seq_len - self.max_seq))\n",
    "            end = start + self.max_seq\n",
    "            q[:] = q_[start: end]\n",
    "            qa[:] = qa_[start: end]\n",
    "            qt[:] = qt_[start: end]\n",
    "            qe[:] = qe_[start: end]\n",
    "            qte[:] = qte_[start: end]\n",
    "        else:\n",
    "            start = 0\n",
    "            end = random.randint(2, seq_len)\n",
    "            seq_len = end - start\n",
    "            q[-seq_len:] = q_[0: seq_len]\n",
    "            qa[-seq_len:] = qa_[0: seq_len]\n",
    "            qt[-seq_len:] = qt_[0: seq_len]\n",
    "            qe[-seq_len:] = qe_[0: seq_len]\n",
    "            qte[-seq_len:] = qte_[0: seq_len]\n",
    "\n",
    "        target_id = np.array(q[1:].copy())\n",
    "        label = qa[1:].copy()\n",
    "        ac = np.array(qa[:-1].copy())\n",
    "        ###\n",
    "        te_content_id = qte[1:].copy()\n",
    "        ###\n",
    "        \n",
    "        learn_start_idx = np.where(target_id > 0)[0][0]   # 変更した\n",
    "\n",
    "        part = np.zeros(self.max_seq - 1)\n",
    "        part[learn_start_idx:] = self.q2p[target_id[learn_start_idx:]]   # 変更した\n",
    "\n",
    "        difftime = np.diff(qt.copy())\n",
    "        difftime = np.where(difftime < 0, 300, difftime)\n",
    "        difftime = np.log1p(difftime)\n",
    "\n",
    "        prior_elapsed = qe[1:].copy()\n",
    "        prior_elapsed = np.log1p(prior_elapsed)\n",
    "        prior_elapsed = np.where(np.isnan(prior_elapsed), np.log1p(21), prior_elapsed)\n",
    "\n",
    "        qtg[learn_start_idx:, :] = self.q2tg[target_id[learn_start_idx:]]   # 変更した\n",
    "        \n",
    "        ###\n",
    "        te_content_id = np.where(np.isnan(te_content_id), 0.625164097637492, te_content_id)   # nanmean\n",
    "\n",
    "        avg_u_target = np.zeros(self.max_seq - 1, dtype=float)\n",
    "        ac_latest = ac[learn_start_idx:]\n",
    "        avg_u_target[learn_start_idx:] = ac_latest.cumsum() / (np.arange(len(ac_latest)) + 1)\n",
    "        avg_u_target = np.where(np.isnan(avg_u_target), 0, avg_u_target)\n",
    "        \n",
    "        num_feat = np.vstack([te_content_id, avg_u_target]).T\n",
    "        ###\n",
    "\n",
    "        feat = {\n",
    "            'in_ex': torch.LongTensor(target_id),\n",
    "            'in_dt': torch.FloatTensor(difftime),\n",
    "            'in_el': torch.FloatTensor(prior_elapsed),\n",
    "            'in_tag': torch.LongTensor(qtg),\n",
    "            'in_cat': torch.LongTensor(part),\n",
    "            'in_de': torch.LongTensor(ac),\n",
    "            ###\n",
    "            'num_feat': torch.FloatTensor(num_feat),\n",
    "            ###\n",
    "        }\n",
    "\n",
    "        label = torch.FloatTensor(label)\n",
    "\n",
    "        return feat, label\n",
    "\n",
    "\n",
    "class CustomValidDataset7_2_(Dataset):\n",
    "    def __init__(self, samples, df, q2p, q2tg, q2te, cfg=None):\n",
    "        super(CustomValidDataset7_2_, self).__init__()\n",
    "        self.max_seq = cfg.params.max_seq\n",
    "        self.n_skill = cfg.params.n_skill\n",
    "        self.n_tag = cfg.params.total_tg\n",
    "        self.samples = samples\n",
    "        self.df = df\n",
    "        self.q2p = q2p\n",
    "        self.q2tg = q2tg\n",
    "        ###\n",
    "        self.q2te = q2te\n",
    "        ###\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        row_id = row['row_id']\n",
    "\n",
    "        seq_list = dh.load(f'{VALID_SEQ_DIR}/row_{int(row_id)}.pkl')\n",
    "\n",
    "        difftime = np.array(seq_list[1]) / 60_000.   # ms -> m\n",
    "        difftime = np.where(difftime < 0, 300, difftime)\n",
    "        difftime = np.log1p(difftime)\n",
    "\n",
    "        prior_elapsed = np.array(seq_list[2]) / 1_000.\n",
    "        prior_elapsed = np.log1p(prior_elapsed)\n",
    "        prior_elapsed = np.where(np.isnan(prior_elapsed), np.log1p(21), prior_elapsed)\n",
    "\n",
    "        content_id = np.array(seq_list[0])\n",
    "        learn_start_idx = np.where(content_id > 0)[0][0]   # 変更した\n",
    "        \n",
    "        part = np.zeros(self.max_seq - 1)\n",
    "        part[learn_start_idx:] = self.q2p[content_id[learn_start_idx:]]   # 変更した\n",
    "        \n",
    "        target = np.array(seq_list[3])\n",
    "\n",
    "        qtg = np.zeros((self.max_seq - 1, 6)) + self.n_tag\n",
    "        qtg[learn_start_idx:, :] = self.q2tg[content_id[learn_start_idx:]]   # 変更した\n",
    "        \n",
    "        ###\n",
    "        avg_u_target = np.zeros(self.max_seq - 1, dtype=float)\n",
    "        ac_latest = target[learn_start_idx:]\n",
    "        avg_u_target[learn_start_idx:] = ac_latest.cumsum() / (np.arange(len(ac_latest)) + 1)\n",
    "        avg_u_target = np.where(np.isnan(avg_u_target), 0, avg_u_target)\n",
    "        \n",
    "        te_content_id = np.zeros(self.max_seq - 1)\n",
    "        te_content_id[learn_start_idx:] = self.q2te[content_id[learn_start_idx:]]\n",
    "        te_content_id = np.where(np.isnan(te_content_id), 0.625164097637492, te_content_id)   # nanmean\n",
    "        \n",
    "        num_feat = np.vstack([te_content_id, avg_u_target]).T\n",
    "        ###\n",
    "        \n",
    "        feat = {\n",
    "            'in_ex': torch.LongTensor(content_id),\n",
    "            'in_dt': torch.FloatTensor(difftime),\n",
    "            'in_el': torch.FloatTensor(prior_elapsed),\n",
    "            'in_tag': torch.LongTensor(qtg),\n",
    "            'in_cat': torch.LongTensor(part),\n",
    "            'in_de': torch.LongTensor(target),\n",
    "            ###\n",
    "            'num_feat': torch.FloatTensor(num_feat),\n",
    "            ###\n",
    "        }\n",
    "\n",
    "        if TARGET_COLS[0] in self.df.columns:\n",
    "            label = np.append(target[1:], [row[TARGET_COLS[0]]])\n",
    "            label = torch.FloatTensor(label)\n",
    "            return feat, label\n",
    "        else:\n",
    "            return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/arshadshk/SAINT-pytorch/blob/main/saint.py\n",
    "class Feed_Forward_block(nn.Module):\n",
    "    \"\"\"\n",
    "    out =  Relu( M_out*w1 + b1) *w2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_ff):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "        self.layer2 = nn.Linear(in_features=dim_ff, out_features=dim_ff)\n",
    "\n",
    "    def forward(self, ffn_in):\n",
    "        return self.layer2(F.relu(self.layer1(ffn_in)))\n",
    "\n",
    "\n",
    "class Encoder_block(nn.Module):\n",
    "    \"\"\"\n",
    "    M = SkipConct(Multihead(LayerNorm(Qin;Kin;Vin)))\n",
    "    O = SkipConct(FFN(LayerNorm(M)))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_model, heads_en, total_ex, total_cat, total_tg, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len - 1\n",
    "        self.embd_ex = nn.Embedding(total_ex, embedding_dim=dim_model)\n",
    "        self.embd_cat = nn.Embedding(total_cat + 1, embedding_dim=dim_model)\n",
    "        self.embd_tg = nn.Embedding(total_tg + 1, embedding_dim=dim_model)\n",
    "        self.embd_pos = nn.Embedding(seq_len, embedding_dim=dim_model)\n",
    "        self.dt_fc = nn.Linear(1, dim_model, bias=False)\n",
    "        # self.task_fc = nn.Linear(1, dim_model, bias=False)\n",
    "\n",
    "        self.multi_en = nn.MultiheadAttention(embed_dim=dim_model, num_heads=heads_en)\n",
    "        self.ffn_en = Feed_Forward_block(dim_model)\n",
    "        self.layer_norm1 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, in_ex, in_cat, in_tg, in_dt, first_block=True):\n",
    "        device = in_ex.device\n",
    "\n",
    "        if first_block:\n",
    "            in_ex = self.embd_ex(in_ex)\n",
    "            in_cat = self.embd_cat(in_cat)\n",
    "\n",
    "            in_dt = in_dt.unsqueeze(-1)\n",
    "            in_dt = self.dt_fc(in_dt)\n",
    "\n",
    "            in_tg = self.embd_tg(in_tg)\n",
    "            avg_in_tg_embed = in_tg.mean(dim=2)\n",
    "            max_in_tg_embed = in_tg.max(dim=2).values\n",
    "\n",
    "            # in_task = in_task.unsqueeze(-1)\n",
    "            # in_task = self.task_fc(in_task)\n",
    "\n",
    "            # combining the embedings\n",
    "            # out = in_ex + in_cat + in_dt + (avg_in_tg_embed + max_in_tg_embed) + in_task\n",
    "            out = in_ex + in_cat + in_dt + (avg_in_tg_embed + max_in_tg_embed)\n",
    "        else:\n",
    "            out = in_ex\n",
    "\n",
    "        in_pos = get_pos(self.seq_len, device)\n",
    "        in_pos = self.embd_pos(in_pos)\n",
    "        out = out + in_pos\n",
    "\n",
    "        out = out.permute(1, 0, 2)\n",
    "\n",
    "        # Multihead attention\n",
    "        n, _, _ = out.shape\n",
    "        out = self.layer_norm1(out)\n",
    "        skip_out = out\n",
    "        out, attn_wt = self.multi_en(out, out, out,\n",
    "                                     attn_mask=get_mask(seq_len=n, device=device))\n",
    "        out = out + skip_out\n",
    "\n",
    "        # feed forward\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = self.layer_norm2(out)\n",
    "        skip_out = out\n",
    "        out = self.ffn_en(out)\n",
    "        out = out + skip_out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Decoder_block(nn.Module):\n",
    "    \"\"\"\n",
    "    M1 = SkipConct(Multihead(LayerNorm(Qin;Kin;Vin)))\n",
    "    M2 = SkipConct(Multihead(LayerNorm(M1;O;O)))\n",
    "    L = SkipConct(FFN(LayerNorm(M2)))\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model, total_in, total_exp, heads_de, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len - 1\n",
    "        self.embd_in = nn.Embedding(total_in, embedding_dim=dim_model)\n",
    "        # self.embd_exp = nn.Embedding(total_exp, embedding_dim=dim_model)\n",
    "        self.embd_pos = nn.Embedding(self.seq_len, embedding_dim=dim_model)\n",
    "        self.multi_de1 = nn.MultiheadAttention(embed_dim=dim_model, num_heads=heads_de)\n",
    "        self.multi_de2 = nn.MultiheadAttention(embed_dim=dim_model, num_heads=heads_de)\n",
    "        self.ffn_en = Feed_Forward_block(dim_model)\n",
    "        self.el_fc = nn.Linear(1, dim_model, bias=False)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim_model)\n",
    "        self.layer_norm3 = nn.LayerNorm(dim_model)\n",
    "\n",
    "    def forward(self, in_in, in_el, en_out, first_block=True):\n",
    "        device = in_in.device\n",
    "\n",
    "        if first_block:\n",
    "            in_in = self.embd_in(in_in)\n",
    "\n",
    "            in_el = in_el.unsqueeze(-1)\n",
    "            in_el = self.el_fc(in_el)\n",
    "            # in_exp = self.embd_exp(in_exp)\n",
    "\n",
    "            # out = in_in + in_el + in_exp\n",
    "            out = in_in + in_el\n",
    "        else:\n",
    "            out = in_in\n",
    "\n",
    "        in_pos = get_pos(self.seq_len, device)\n",
    "        in_pos = self.embd_pos(in_pos)\n",
    "        out = out + in_pos\n",
    "\n",
    "        out = out.permute(1, 0, 2)\n",
    "        n, _, _ = out.shape\n",
    "\n",
    "        out = self.layer_norm1(out)\n",
    "        skip_out = out\n",
    "        out, attn_wt = self.multi_de1(out, out, out,\n",
    "                                      attn_mask=get_mask(seq_len=n, device=device))\n",
    "        out = skip_out + out\n",
    "\n",
    "        en_out = en_out.permute(1, 0, 2)\n",
    "        en_out = self.layer_norm2(en_out)\n",
    "        skip_out = out\n",
    "        out, attn_wt = self.multi_de2(out, en_out, en_out,\n",
    "                                      attn_mask=get_mask(seq_len=n, device=device))\n",
    "        out = out + skip_out\n",
    "\n",
    "        out = out.permute(1, 0, 2)\n",
    "        out = self.layer_norm3(out)\n",
    "        skip_out = out\n",
    "        out = self.ffn_en(out)\n",
    "        out = out + skip_out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def get_mask(seq_len, device):\n",
    "    mask = torch.from_numpy(np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)).to(device)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_pos(seq_len, device):\n",
    "    # use sine positional embeddinds\n",
    "    return torch.arange(seq_len, device=device).unsqueeze(0)\n",
    "\n",
    "\n",
    "class SAINT(nn.Module):\n",
    "    def __init__(self, dim_model, num_en, num_de, heads_en, total_ex, total_cat, total_tg, total_in, total_exp,\n",
    "                      heads_de, seq_len, num_fc_in_dim=2, num_fc_out_dim=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_en = num_en\n",
    "        self.num_de = num_de\n",
    "\n",
    "        self.encoder = get_clones(Encoder_block(dim_model, heads_en, total_ex, total_cat, total_tg, seq_len), num_en)\n",
    "        self.decoder = get_clones(Decoder_block(dim_model, total_in, total_exp, heads_de, seq_len), num_de)\n",
    "\n",
    "#         self.out = nn.Linear(in_features=dim_model, out_features=1)\n",
    "        \n",
    "        self.num_fc = nn.Linear(in_features=num_fc_in_dim, out_features=num_fc_out_dim)\n",
    "        self.out_fc1 = nn.Linear(in_features=dim_model, out_features=num_fc_out_dim)\n",
    "        self.out_fc2 = nn.Linear(in_features=num_fc_out_dim * 2, out_features=1)\n",
    "\n",
    "    def forward(self, feat):\n",
    "        in_ex = feat['in_ex']\n",
    "        in_dt = feat['in_dt']\n",
    "        in_el = feat['in_el']\n",
    "        in_tg = feat['in_tag']\n",
    "        in_cat = feat['in_cat']\n",
    "        in_in = feat['in_de']\n",
    "        ###\n",
    "        num_feat = feat['num_feat']\n",
    "        ###\n",
    "\n",
    "        first_block = True\n",
    "        for x in range(self.num_en):\n",
    "            if x >= 1:\n",
    "                first_block = False\n",
    "            in_ex = self.encoder[x](in_ex, in_cat, in_tg, in_dt, first_block=first_block)\n",
    "            in_cat = in_ex\n",
    "\n",
    "        first_block = True\n",
    "        for x in range(self.num_de):\n",
    "            if x >= 1:\n",
    "                first_block = False\n",
    "            in_in = self.decoder[x](in_in, in_el, en_out=in_ex, first_block=first_block)\n",
    "\n",
    "#         in_in = self.out(in_in)\n",
    "        num_feat = self.num_fc(num_feat)\n",
    "        in_in = self.out_fc1(in_in)\n",
    "        in_in = torch.cat([in_in, num_feat], dim=2)\n",
    "        in_in = self.out_fc2(in_in)\n",
    "    \n",
    "        return in_in.squeeze(-1)\n",
    "\n",
    "def replace_fc(model, cfg):\n",
    "    return model\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.base_model = SAINT(**cfg['model']['params'])\n",
    "        self.model = replace_fc(self.base_model, cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(model, train_loader, criterion, optimizer, mb):\n",
    "    model.train()\n",
    "    avg_loss = 0.\n",
    "\n",
    "    for feats, targets in progress_bar(train_loader, parent=mb):\n",
    "        if type(feats) == dict:\n",
    "            for k, v in feats.items():\n",
    "                feats[k] = v.to(device)\n",
    "        else:\n",
    "            feats = feats.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(feats)\n",
    "\n",
    "        loss = criterion(preds, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    del feats, targets; gc.collect()\n",
    "    return model, avg_loss\n",
    "\n",
    "def _val_epoch(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_preds = np.zeros((len(valid_loader.dataset), cfg.model.n_classes))\n",
    "\n",
    "    avg_val_loss = 0.\n",
    "    valid_batch_size = valid_loader.batch_size\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (feats, targets) in enumerate(valid_loader):\n",
    "            if type(feats) == dict:\n",
    "                for k, v in feats.items():\n",
    "                    feats[k] = v.to(device)\n",
    "            else:\n",
    "                feats = feats.to(device)\n",
    "\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            preds = model(feats)\n",
    "\n",
    "            loss = criterion(preds, targets)\n",
    "\n",
    "            preds = preds[:, -1]\n",
    "            valid_preds[i * valid_batch_size: (i + 1) * valid_batch_size, :] = preds.sigmoid().cpu().detach().numpy().reshape(-1, 1)\n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "    return valid_preds, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp(cfg):\n",
    "    \n",
    "    train_df['step'] = train_df.groupby('user_id').cumcount() // STEP_LENGTH\n",
    "    train_df['user_step_id'] = train_df['user_id'].astype(str) + '__' + train_df['step'].astype(str)\n",
    "\n",
    "    oof = np.zeros((len(train_df), cfg.model.n_classes))\n",
    "    cv = 0\n",
    "    col = 'fold_0'\n",
    "\n",
    "    trn_x, val_x = train_df[fold_df[col] == 0], train_df[fold_df[col] > 0]\n",
    "    val_y = target_df[fold_df[col] > 0].values\n",
    "\n",
    "    usecols = ['user_id', 'content_id', 'timestamp', 'prior_question_elapsed_time',\n",
    "                    'answered_correctly', 'te_content_id_by_answered_correctly']\n",
    "    group = (trn_x[usecols]\n",
    "             .groupby('user_id')\n",
    "             .apply(lambda r: (r['content_id'].values,\n",
    "                                        r['answered_correctly'].values,\n",
    "                                        r['timestamp'].values,\n",
    "                                        r['prior_question_elapsed_time'].values,\n",
    "                                        r['te_content_id_by_answered_correctly'].values)))\n",
    "\n",
    "    dataset = CustomTrainDataset7_2_(samples=group, df=trn_x, q2p=q2p, q2tg=q2tg, q2te=q2te, cfg=cfg.data.train)\n",
    "    train_loader = DataLoader(dataset, **cfg.data.train.loader)\n",
    "\n",
    "    dataset = CustomValidDataset7_2_(samples=group, df=val_x, q2p=q2p, q2tg=q2tg, q2te=q2te, cfg=cfg.data.valid)\n",
    "    valid_loader = DataLoader(dataset, **cfg.data.valid.loader)\n",
    "\n",
    "    is_train = True\n",
    "    model = CustomModel(cfg)\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    loss_func = getattr(nn, cfg.loss.name)(**cfg.loss.params)\n",
    "    metric_func = auc\n",
    "    optimizer = getattr(torch.optim, cfg.optimizer.name)(params=model.parameters(), **cfg.optimizer.params)\n",
    "    scheduler = getattr(torch.optim.lr_scheduler, cfg.scheduler.name)(\n",
    "        optimizer,\n",
    "        **cfg.scheduler.params,\n",
    "    )\n",
    "\n",
    "    best_epoch = -1\n",
    "    best_val_score = -np.inf\n",
    "    mb = master_bar(range(cfg.model.epochs))\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_score_list = []\n",
    "\n",
    "    for epoch in mb:\n",
    "        start_time = time.time()\n",
    "\n",
    "        with detect_anomaly():\n",
    "            model, avg_loss = _train_epoch(model, train_loader, loss_func, optimizer, mb)\n",
    "\n",
    "        valid_preds, avg_val_loss = _val_epoch(model, valid_loader, loss_func)\n",
    "\n",
    "        val_score = metric_func(val_y, valid_preds)\n",
    "\n",
    "        train_loss_list.append(avg_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_score_list.append(val_score)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        mb.write(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.6f}  avg_val_loss: {avg_val_loss:.6f} val_score: {val_score:.6f} time: {elapsed:.0f}s')\n",
    "\n",
    "        if val_score > best_val_score:\n",
    "            best_epoch = epoch + 1\n",
    "            best_val_score = val_score\n",
    "            best_valid_preds = valid_preds\n",
    "            if cfg.model.multi_gpu:\n",
    "                best_model = model.module.state_dict()\n",
    "            else:\n",
    "                best_model = model.state_dict()\n",
    "#             torch.save(best_model, './seq50_step75_model.pt')\n",
    "\n",
    "    oof[val_x.index, :] = best_valid_preds\n",
    "    cv += best_val_score * fold_df[col].max()\n",
    "    \n",
    "#     np.save('./seq50_step75_oof.npy', oof)\n",
    "\n",
    "#     torch.save(best_model, f'{SAVE_DIR}/{EXP_NAME}_{run_id}/{run_id}_weight_best.pt')\n",
    "#     shutil.copy('pg_transformer20_3_on_note_1.ipynb', f'{SAVE_DIR}/{EXP_NAME}_{run_id}/pg_transformer20_3_on_note_1.ipynb')\n",
    "#     shutil.copy(f'../exp/000_tran/{model_name}.yml', f'{SAVE_DIR}/{EXP_NAME}_{run_id}/config.yml')\n",
    "\n",
    "    print(f'\\nEpoch {best_epoch} - val_score: {best_val_score:.6f}')\n",
    "\n",
    "    print('\\n\\n===================================\\n')\n",
    "    print(f'CV: {cv:.6f}')\n",
    "    print('\\n===================================\\n\\n')\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.393206  avg_val_loss: 0.419488 val_score: 0.773363 time: 1020s\n",
      "Epoch 2 - avg_train_loss: 0.383440  avg_val_loss: 0.415616 val_score: 0.779901 time: 1017s\n",
      "█\r"
     ]
    }
   ],
   "source": [
    "cv = exp(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- デフォルト<br>\n",
    "Epoch 1 - avg_train_loss: 0.418306  avg_val_loss: 0.420112 val_score: 0.772268 time: 1200s <br>\n",
    "Epoch 2 - avg_train_loss: 0.405379  avg_val_loss: 0.416274 val_score: 0.778598 time: 1099s <br>\n",
    "Epoch 3 - avg_train_loss: 0.402293  avg_val_loss: 0.415704 val_score: 0.780514 time: 1100s <br>\n",
    "Epoch 4 - avg_train_loss: 0.400122  avg_val_loss: 0.411587 val_score: 0.784737 time: 1100s <br>\n",
    "Epoch 5 - avg_train_loss: 0.397836  avg_val_loss: 0.410092 val_score: 0.787489 time: 1101s <br>\n",
    "\n",
    "\n",
    "- tag高速化 & part修正 <br>\n",
    "Epoch 1 - avg_train_loss: 0.415494  avg_val_loss: 0.420086 val_score: 0.772824 time: 976s <br>\n",
    "Epoch 2 - avg_train_loss: 0.402696  avg_val_loss: 0.416425 val_score: 0.778567 time: 972s <br>\n",
    "Epoch 3 - avg_train_loss: 0.399676  avg_val_loss: 0.414455 val_score: 0.782042 time: 973s <br>\n",
    "Epoch 4 - avg_train_loss: 0.397366  avg_val_loss: 0.411210 val_score: 0.785679 time: 974s <br>\n",
    "Epoch 5 - avg_train_loss: 0.395128  avg_val_loss: 0.409938 val_score: 0.787973 time: 972s <br>\n",
    "\n",
    "\n",
    "- seq_len=50 <br>\n",
    "Epoch 1 - avg_train_loss: 0.502999  avg_val_loss: 0.471466 val_score: 0.769687 time: 859s <br>\n",
    "Epoch 2 - avg_train_loss: 0.489230  avg_val_loss: 0.468640 val_score: 0.775218 time: 858s <br>\n",
    "Epoch 3 - avg_train_loss: 0.486275  avg_val_loss: 0.466067 val_score: 0.777586 time: 858s <br>\n",
    "Epoch 4 - avg_train_loss: 0.484485  avg_val_loss: 0.464312 val_score: 0.780000 time: 856s <br>\n",
    "Epoch 5 - avg_train_loss: 0.483002  avg_val_loss: 0.463606 val_score: 0.781718 time: 859s <br>\n",
    "\n",
    "\n",
    "- add te, avg_u_target <br>\n",
    "Epoch 1 - avg_train_loss: 0.411664  avg_val_loss: 0.419125 val_score: 0.774234 time: 1006s <br>\n",
    "Epoch 2 - avg_train_loss: 0.401464  avg_val_loss: 0.414941 val_score: 0.780295 time: 1006s <br>\n",
    "Epoch 3 - avg_train_loss: 0.398836  avg_val_loss: 0.413717 val_score: 0.782442 time: 1007s <br>\n",
    "Epoch 4 - avg_train_loss: 0.397053  avg_val_loss: 0.413128 val_score: 0.785359 time: 1007s <br>\n",
    "Epoch 5 - avg_train_loss: 0.395297  avg_val_loss: 0.409641 val_score: 0.788182 time: 1007s <br>\n",
    "Epoch 6 - avg_train_loss: 0.393423  avg_val_loss: 0.408399 val_score: 0.790023 time: 1006s <br>\n",
    "Epoch 7 - avg_train_loss: 0.392346  avg_val_loss: 0.407184 val_score: 0.791622 time: 1007s <br>\n",
    "Epoch 8 - avg_train_loss: 0.391387  avg_val_loss: 0.407105 val_score: 0.792346 time: 1006s <br>\n",
    "Epoch 9 - avg_train_loss: 0.390765  avg_val_loss: 0.405870 val_score: 0.793542 time: 1013s <br>\n",
    "Epoch 10 - avg_train_loss: 0.390244  avg_val_loss: 0.406136 val_score: 0.793739 time: 1007s <br>\n",
    "Epoch 11 - avg_train_loss: 0.389793  avg_val_loss: 0.405646 val_score: 0.794341 time: 1007s <br>\n",
    "Epoch 12 - avg_train_loss: 0.389439  avg_val_loss: 0.405179 val_score: 0.793943 time: 1007s <br>\n",
    "Epoch 13 - avg_train_loss: 0.389116  avg_val_loss: 0.404685 val_score: 0.794862 time: 1007s <br>\n",
    "Epoch 14 - avg_train_loss: 0.388748  avg_val_loss: 0.404421 val_score: 0.795003 time: 1007s <br>\n",
    "Epoch 15 - avg_train_loss: 0.388514  avg_val_loss: 0.404539 val_score: 0.794840 time: 1008s <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if debug:\n",
    "#     train_df = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype=DTYPE, nrows=10**6)\n",
    "# else:\n",
    "#     train_df = pd.read_csv(f'{INPUT_DIR}/train.csv', dtype=DTYPE)\n",
    "\n",
    "# folds = pd.read_feather(f'{FOLD_DIR}/train_folds_{FOLD_NAME}_v2_{RANDOM_STATE}.feather')\n",
    "# valid_idx = folds[folds.val == 1]['index'].values\n",
    "# if debug:\n",
    "#     valid_idx = valid_idx[np.where(valid_idx < len(train_df))]\n",
    "\n",
    "# fold_df = pd.DataFrame(index=range(len(train_df)))\n",
    "# fold_df['fold_0'] = 0\n",
    "# fold_df.loc[valid_idx, 'fold_0'] += 1\n",
    "\n",
    "# drop_idx = train_df[train_df.content_type_id != 0].index\n",
    "# train_df = train_df.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "# fold_df = fold_df.drop(drop_idx, axis=0).reset_index(drop=True)\n",
    "\n",
    "# oof = np.load('./seq50_step75_oof.npy')\n",
    "# val_idx = fold_df[fold_df['fold_0'] == 1].index\n",
    "# oof = oof[val_idx\n",
    "          \n",
    "# preds_df = pd.DataFrame(oof, columns=['preds'])\n",
    "# preds_df.to_csv('./preds_val_based_seq50_step75.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "miniconda3-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
