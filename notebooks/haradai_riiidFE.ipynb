{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from bitarray import bitarray\n",
    "import pickle5\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "sys.path.append('../src')\n",
    "import const\n",
    "from utils import (\n",
    "    seed_everything,\n",
    "    Timer,\n",
    "    reduce_mem_usage,\n",
    "    # load_from_pkl,\n",
    "#     save_as_pkl\n",
    ")\n",
    "\n",
    "from riiid_fe import riiidFE\n",
    "\n",
    "# from preprocess_func import (\n",
    "#     prep_tags,\n",
    "#     part2lr,\n",
    "#     merge_questions_w_prep,\n",
    "#     prep_base,\n",
    "#     add_modified_target_based_on_user_answer,\n",
    "#     make_repeat_table,\n",
    "#     make_elapsed_time_table\n",
    "# )\n",
    "\n",
    "# from model import riiidFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/home/myaun/catboost/catboost/python-package')\n",
    "# import catboost\n",
    "# print('CAT Version', catboost.__version__)\n",
    "\n",
    "INPUT_DIR = const.INPUT_DATA_DIR\n",
    "FOLD_DIR = '../folds'\n",
    "# EXP_DIR = '../exp'\n",
    "# EXP_CONFIG = '001'\n",
    "# sys.path.append(f'{EXP_DIR}/{EXP_CONFIG}')\n",
    "import config\n",
    "\n",
    "RANDOM_STATE = 46\n",
    "FOLD_NAME = config.FOLD_NAME\n",
    "\n",
    "use_features = config.use_features\n",
    "FE_MODEL_PARAMS = config.FE_MODEL_PARAMS\n",
    "CAT_PARAMS = config.CAT_PARAMS\n",
    "\n",
    "w2v_feature_path = config.w2v_feature_path\n",
    "g_feature_path = config.g_feature_path\n",
    "corr_g_feature_path = config.corr_g_feature_path\n",
    "ge_dw_feature_path = config.ge_dw_feature_path\n",
    "ge_s2v_feature_path = config.ge_s2v_feature_path\n",
    "\n",
    "SAVE_EXTACTED_FEATURES = False\n",
    "\n",
    "# EXP_NAME = f'{FOLD_NAME}__CAT'\n",
    "# if not os.path.exists(f'../features/{EXP_NAME}/'):\n",
    "#     os.mkdir(f'../features/{EXP_NAME}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_tags(x):\n",
    "    return [int(i) for i in x.split()]\n",
    "\n",
    "\n",
    "def part2lr(x):\n",
    "    if x in [1, 2, 3, 4]:\n",
    "        return 0\n",
    "    if x in [5, 6, 7]:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def merge_questions_w_prep(df, question):\n",
    "    df = pd.merge(df, question[['question_id', 'part', 'tags', 'lr']], left_on='content_id', right_on='question_id', how='left')\n",
    "    df['part'] = df['part'].fillna(0.0).astype('int8')\n",
    "    df['tags'] = df['tags'].fillna('0')\n",
    "    return df\n",
    "\n",
    "\n",
    "def prep_base(df):\n",
    "    # df['prior_question_elapsed_time_cat'] = pd.cut(df['prior_question_elapsed_time'], [0, 10000, 30000, 50000, 100000, 300000], labels=False).fillna(5).astype('int8')\n",
    "    df['prior_question_elapsed_time'] = df['prior_question_elapsed_time'].fillna(-1).astype(int)\n",
    "    df['prior_question_had_explanation'] = df['prior_question_had_explanation'].fillna(False).astype('int8')\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_modified_target_based_on_user_answer(X_tra_wo_lec, question):\n",
    "    # add modified target values 'weighted_answered_correctly', 'weighted_score'\n",
    "    agg = X_tra_wo_lec.groupby(['content_id', 'user_answer'])['user_answer'].agg(['count']).reset_index()\n",
    "    ccnt = X_tra_wo_lec['content_id'].value_counts().reset_index()\n",
    "    ccnt.columns = ['content_id', 'c_count']\n",
    "\n",
    "    agg = pd.merge(agg, ccnt, on='content_id', how='left')\n",
    "    agg['choice_rate'] = agg['count'] / agg['c_count']\n",
    "    agg = pd.merge(agg, question[['question_id', 'correct_answer']], right_on='question_id', left_on='content_id', how='left')\n",
    "    agg['answered_correctly'] = (agg['user_answer'] == agg['correct_answer']) * 1\n",
    "\n",
    "    # 'weighted_answered_correctly', 難しい問題に強い重み\n",
    "    agg['weighted_answered_correctly'] = (1.0 - agg['answered_correctly'] * agg['choice_rate']) * agg['answered_correctly']\n",
    "\n",
    "    # 'weighted_score', 不正解度合いによって重み付け\n",
    "    agg2 = pd.DataFrame()\n",
    "    for cid, tmp in tqdm(agg.groupby('content_id')):\n",
    "        x = tmp[tmp.answered_correctly == 1]['choice_rate'].values.tolist() + tmp[tmp.answered_correctly != 1].sort_values('choice_rate', ascending=False)['choice_rate'].values.tolist()\n",
    "        weighted_score = [1.0]\n",
    "        for i in range(len(tmp) - 1):\n",
    "            weighted_score.append(weighted_score[-1] - x[i])\n",
    "        tmp['weighted_score'] = weighted_score\n",
    "        agg2 = pd.concat([agg2, tmp])\n",
    "    return agg2\n",
    "\n",
    "\n",
    "def make_elapsed_time_table(X_tra_wo_lec, FOLD_NAME):\n",
    "    rows = []\n",
    "    for uid, udf in tqdm(X_tra_wo_lec[['user_id', 'timestamp', 'content_id', 'part', 'prior_question_had_explanation', 'prior_question_elapsed_time']].groupby('user_id')):\n",
    "        udf['question_elapsed_time'] = udf['prior_question_elapsed_time'].shift(-1)\n",
    "        udf = udf[~udf.question_elapsed_time.isna()]\n",
    "        rows.extend(udf.values)\n",
    "\n",
    "    et_table = pd.DataFrame(rows, columns=['user_id', 'timestamp', 'content_id', 'part', 'prior_question_had_explanation', 'prior_question_elapsed_time', 'question_elapsed_time'])\n",
    "    et_table['content_id'] = et_table['content_id'].astype(int)\n",
    "    et_table['part'] = et_table['part'].astype(int)\n",
    "    et_table['prior_question_had_explanation'] = et_table['prior_question_had_explanation'].astype(int)\n",
    "    et_table['question_elapsed_time'] = et_table['question_elapsed_time'].astype(int)\n",
    "#     et_table.to_feather(f'../save/et_table_{FOLD_NAME}.feather')\n",
    "    return et_table\n",
    "\n",
    "\n",
    "def make_repeat_table(X_tra_wo_lec, FOLD_NAME):\n",
    "    repeat_idx = []\n",
    "    question_u_dict = {}\n",
    "    for r, uid, cid, ans in tqdm(X_tra_wo_lec[['row_id', 'user_id', 'content_id', 'answered_correctly']].values):\n",
    "        if uid not in question_u_dict:\n",
    "            question_u_dict[uid] = np.zeros(13523, dtype=np.uint8)\n",
    "        if question_u_dict[uid][cid] > 0:\n",
    "            repeat_idx.append(r)\n",
    "        question_u_dict[uid][cid] += 1\n",
    "    repeat = X_tra_wo_lec[X_tra_wo_lec.row_id.isin(repeat_idx)]\n",
    "    repeat = repeat.reset_index(drop=True)\n",
    "#     repeat.to_feather(f'../save/repeat_{FOLD_NAME}.feather')\n",
    "    return repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fix seed RANDOM_STATE:46] done in 0 s (Total: 0.00 sec)\n",
      "[read data] done in 4 s (Total: 4.27 sec)\n",
      "[Data split] done in 0 s (Total: 4.27 sec)\n"
     ]
    }
   ],
   "source": [
    "t = Timer()\n",
    "with t.timer(f'fix seed RANDOM_STATE:{RANDOM_STATE}'):\n",
    "    seed_everything(RANDOM_STATE)\n",
    "    \n",
    "with t.timer(f'read data'):\n",
    "    X_tra = pd.read_pickle(f'{FOLD_DIR}/cv1_train.pickle').iloc[:100_000]\n",
    "    X_val = pd.read_pickle(f'{FOLD_DIR}/cv1_valid.pickle').iloc[:2_500]\n",
    "    question = pd.read_csv(f'{INPUT_DIR}/questions.csv')\n",
    "    lecture = pd.read_csv(f'{INPUT_DIR}/lectures.csv')\n",
    "    \n",
    "with t.timer(f'Data split'):\n",
    "    X_tra_wo_lec = X_tra[X_tra['content_type_id'] == False]\n",
    "    X_val_wo_lec = X_val[X_val['content_type_id'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9726/9726 [00:28<00:00, 347.25it/s]\n",
      "100%|██████████| 2210/2210 [00:02<00:00, 1001.74it/s]\n",
      "100%|██████████| 99067/99067 [00:00<00:00, 178488.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Preprocess] done in 31 s (Total: 35.68 sec)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with t.timer(f'Preprocess'):\n",
    "    # preprocess\n",
    "    question['lr'] = question['part'].apply(part2lr)   # part1 ~ 4 → 0, part5 ~ 7 → 1に変換\n",
    "\n",
    "    X_tra = merge_questions_w_prep(X_tra, question)   # merge & fillna\n",
    "    X_val = merge_questions_w_prep(X_val, question)\n",
    "\n",
    "    X_tra = prep_base(X_tra)   # fillna\n",
    "    X_val = prep_base(X_val)\n",
    "\n",
    "    X_tra['tags'] = X_tra['tags'].fillna('0').apply(prep_tags)  # 0埋め & tagを[int, int, ...]の形に変換\n",
    "    X_val['tags'] = X_val['tags'].fillna('0').apply(prep_tags)\n",
    "\n",
    "    X_tra_wo_lec = prep_base(X_tra_wo_lec)   # fillna\n",
    "    X_val_wo_lec = prep_base(X_val_wo_lec)\n",
    "\n",
    "    X_tra_wo_lec = pd.merge(X_tra_wo_lec, question[['question_id', 'lr', 'part', 'tags']], left_on='content_id', right_on='question_id', how='left')\n",
    "    X_val_wo_lec = pd.merge(X_val_wo_lec, question[['question_id', 'lr', 'part', 'tags']], left_on='content_id', right_on='question_id', how='left')\n",
    "\n",
    "    agg = add_modified_target_based_on_user_answer(X_tra_wo_lec, question)   # content_idの選択肢ごとの重みdataframeを作成\n",
    "    X_tra_wo_lec = pd.merge(X_tra_wo_lec, agg[['content_id', 'user_answer', 'weighted_answered_correctly', 'weighted_score']], on=['content_id', 'user_answer'], how='left')\n",
    "\n",
    "    if os.path.exists(f'../save/et_table_{FOLD_NAME}.feather'):\n",
    "        logging.info(f'skip make_elapsed_time_table')\n",
    "        et_table = pd.read_feather(f'../save/et_table_{FOLD_NAME}.feather')\n",
    "    else:\n",
    "        et_table = make_elapsed_time_table(X_tra_wo_lec, FOLD_NAME)   # 回答時間を含めたdataframeの作成\n",
    "\n",
    "    if os.path.exists(f'../save/rp_table_{FOLD_NAME}.feather'):\n",
    "        logging.info(f'skip make_repeat_table')\n",
    "        rp_table = pd.read_feather(f'../save/rp_table_{FOLD_NAME}.feather')\n",
    "    else:\n",
    "        rp_table = make_repeat_table(X_tra_wo_lec, FOLD_NAME)   #同じquestionの回答が2回目以降の部分だけを切り取ったdataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[reduce Mem] done in 0 s (Total: 35.72 sec)\n"
     ]
    }
   ],
   "source": [
    "# メモリ削減\n",
    "with t.timer(f'reduce Mem'):\n",
    "    X_tra = reduce_mem_usage(X_tra)\n",
    "    X_val = reduce_mem_usage(X_val)\n",
    "    X_tra_wo_lec = reduce_mem_usage(X_tra_wo_lec)\n",
    "    X_val_wo_lec = reduce_mem_usage(X_val_wo_lec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Num: 130\n",
      "0.5724711558844015 0.1862932 0.5648155\n",
      "0.7691119691119691 0.30617327551013307 0.5639839827949269\n",
      "[init FE Model] done in 0 s (Total: 35.76 sec)\n"
     ]
    }
   ],
   "source": [
    "with t.timer(f'init FE Model'):\n",
    "    riiidFE = riiidFE(FE_MODEL_PARAMS)\n",
    "    riiidFE.set_use_features(use_features)\n",
    "    riiidFE.set_train_mn(X_tra_wo_lec)\n",
    "    riiidFE.set_repeat_mn(rp_table)\n",
    "    riiidFE.set_cat_te_dict(X_tra_wo_lec, question)   # target_encordingなどようの辞書を準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:00<00:24, 4068.19it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "bitarray index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-89735e55a9f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'FE - loop features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muser_feat_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mriiidFE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_user_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX_tra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naoki/kaggle/kaggle-riiid/notebooks/riiid_fe.py\u001b[0m in \u001b[0;36madd_user_feats\u001b[0;34m(self, df, add_feat, update_dict, val)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0muser_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_rec_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0madd_user_latest_record_feat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0muser_ques_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion_u_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/naoki/kaggle/kaggle-riiid/notebooks/riiid_fe.py\u001b[0m in \u001b[0;36madd_user_latest_record_feat\u001b[0;34m(cnt, user_records)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                         \u001b[0mseq2dec_w5\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m                         \u001b[0mseq2dec_w7\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                         \u001b[0mseq2dec_w10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: bitarray index out of range"
     ]
    }
   ],
   "source": [
    "with t.timer(f'FE - loop features'):\n",
    "    user_feat_df = riiidFE.add_user_feats(X_tra, add_feat=True, update_dict=True, val=False)\n",
    "    del X_tra\n",
    "    gc.collect()\n",
    "\n",
    "    X_tra_wo_lec = pd.concat([X_tra_wo_lec, user_feat_df], axis=1)\n",
    "\n",
    "    del user_feat_df\n",
    "    gc.collect()\n",
    "    X_tra_wo_lec = reduce_mem_usage(X_tra_wo_lec)\n",
    "\n",
    "    # add tra only feaures\n",
    "#     smooth = 10\n",
    "#     tar = 'answered_correctly'\n",
    "#     col_list = [\n",
    "#         ['prev_part_s1', 'part'],\n",
    "#         ['prev_part_s2', 'prev_part_s1', 'part'],\n",
    "#         ['prev_part_s3', 'prev_part_s2', 'prev_part_s1', 'part'],\n",
    "#         # ['prev_question_id_s1', 'question_id'],\n",
    "#         # ['prev_question_id_s2', 'prev_question_id_s1', 'question_id'],\n",
    "#     ]\n",
    "#     for col in col_list:\n",
    "#         tra_te_dict = pd.read_feather(f'../save/features_{FOLD_NAME}/{col[-1]}_sequence_s{len(col)-1}_tra_te_sm{smooth}.feather')\n",
    "#         X_tra_wo_lec = pd.merge(X_tra_wo_lec, tra_te_dict, on=['row_id'], how='left')\n",
    "\n",
    "#     # val only flag\n",
    "    user_feat_df = riiidFE.add_user_feats(X_val, add_feat=True, update_dict=True, val=True)\n",
    "    del X_val\n",
    "    gc.collect()\n",
    "\n",
    "    X_val_wo_lec = pd.concat([X_val_wo_lec, user_feat_df], axis=1)\n",
    "\n",
    "    del user_feat_df\n",
    "    gc.collect()\n",
    "    X_val_wo_lec = reduce_mem_usage(X_val_wo_lec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bitarray('0001000011')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bitarray(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tra_wo_lec.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tra_wo_lec['timestamp_lag_diff_rolling10_median_each_user'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(10)[-7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_wo_lec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riiidFE.user_prev_session_short_ts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     # mlflowの準備\n",
    "#     mlflow.set_experiment(EXP_NAME)\n",
    "#     mlflow.start_run()\n",
    "#     run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "#     # ディレクトリとlogの準備\n",
    "#     if not os.path.exists(f'../save/{EXP_NAME}_{run_id}/'):\n",
    "#         os.mkdir(f'../save/{EXP_NAME}_{run_id}/')\n",
    "#         os.mkdir(f'../save/{EXP_NAME}_{run_id}/features')\n",
    "#     logging.basicConfig(filename=f'../save/{EXP_NAME}_{run_id}/logger.log', level=logging.INFO)\n",
    "\n",
    "#     # seed値を固定\n",
    "#     t = Timer()\n",
    "#     with t.timer(f'fix seed RANDOM_STATE:{RANDOM_STATE}'):\n",
    "#         seed_everything(RANDOM_STATE)\n",
    "\n",
    "#     # 生データの読み込み\n",
    "#     with t.timer(f'read data'):\n",
    "#         folds = pd.read_feather(f'{FOLD_DIR}/train_folds_{FOLD_NAME}_v2.feather')\n",
    "#         train = pd.read_feather(f'{INPUT_DIR}/train_v2.feather')\n",
    "#         question = pd.read_csv(f'{INPUT_DIR}/questions.csv')\n",
    "#         lecture = pd.read_csv(f'{INPUT_DIR}/lectures.csv')\n",
    "    \n",
    "#     # 生データの読み込み & 講義の削除\n",
    "#     with t.timer(f'Data split'):\n",
    "#         X_tra = train[train.row_id.isin(folds[folds.val == 0].row_id)]\n",
    "#         X_val = train[train.row_id.isin(folds[folds.val == 1].row_id)]\n",
    "#         X_tra_wo_lec = X_tra[X_tra.content_type_id == 0]\n",
    "#         X_val_wo_lec = X_val[X_val.content_type_id == 0]\n",
    "#         logging.info(f'X_tra:{len(X_tra)}, X_val:{len(X_val)}, X_tra_wo_lec{len(X_tra_wo_lec),}, X_val_wo_lec:{len(X_val_wo_lec)}')\n",
    "\n",
    "#     del train\n",
    "#     del folds\n",
    "#     gc.collect()\n",
    "    \n",
    "#     # 前処理\n",
    "#     with t.timer(f'Preprocess'):\n",
    "\n",
    "#         # preprocess\n",
    "#         question['lr'] = question['part'].apply(part2lr)   # part1 ~ 4 → 0, part5 ~ 7 → 1に変換\n",
    "\n",
    "#         X_tra = merge_questions_w_prep(X_tra, question)   # merge & fillna\n",
    "#         X_val = merge_questions_w_prep(X_val, question)\n",
    "\n",
    "#         X_tra = prep_base(X_tra)   # fillna\n",
    "#         X_val = prep_base(X_val)\n",
    "\n",
    "#         X_tra['tags'] = X_tra['tags'].fillna('0').apply(prep_tags)  # 0埋め & tagを[int, int, ...]の形に変換\n",
    "#         X_val['tags'] = X_val['tags'].fillna('0').apply(prep_tags)\n",
    "\n",
    "#         X_tra_wo_lec = prep_base(X_tra_wo_lec)   # fillna\n",
    "#         X_val_wo_lec = prep_base(X_val_wo_lec)\n",
    "\n",
    "#         X_tra_wo_lec = pd.merge(X_tra_wo_lec, question[['question_id', 'lr', 'part', 'tags']], left_on='content_id', right_on='question_id', how='left')\n",
    "#         X_val_wo_lec = pd.merge(X_val_wo_lec, question[['question_id', 'lr', 'part', 'tags']], left_on='content_id', right_on='question_id', how='left')\n",
    "\n",
    "#         agg = add_modified_target_based_on_user_answer(X_tra_wo_lec, question)   # content_idの選択肢ごとの重みdataframeを作成\n",
    "#         X_tra_wo_lec = pd.merge(X_tra_wo_lec, agg[['content_id', 'user_answer', 'weighted_answered_correctly', 'weighted_score']], on=['content_id', 'user_answer'], how='left')\n",
    "\n",
    "#         if os.path.exists(f'../save/et_table_{FOLD_NAME}.feather'):\n",
    "#             logging.info(f'skip make_elapsed_time_table')\n",
    "#             et_table = pd.read_feather(f'../save/et_table_{FOLD_NAME}.feather')\n",
    "#         else:\n",
    "#             et_table = make_elapsed_time_table(X_tra_wo_lec, FOLD_NAME)   # 回答時間を含めたdataframeの作成\n",
    "\n",
    "#         if os.path.exists(f'../save/rp_table_{FOLD_NAME}.feather'):\n",
    "#             logging.info(f'skip make_repeat_table')\n",
    "#             rp_table = pd.read_feather(f'../save/rp_table_{FOLD_NAME}.feather')\n",
    "#         else:\n",
    "#             rp_table = make_repeat_table(X_tra_wo_lec, FOLD_NAME)   #同じquestionの回答が2回目以降の部分だけを切り取ったdataframe\n",
    "\n",
    "#         w2v_features = pd.read_feather(w2v_feature_path)\n",
    "#         g_features = pd.read_feather(g_feature_path)\n",
    "#         corr_g_features = pd.read_feather(corr_g_feature_path)\n",
    "#         ge_dw_features = pd.read_csv(ge_dw_feature_path)\n",
    "#         ge_s2v_features = pd.read_csv(ge_s2v_feature_path)\n",
    "\n",
    "#     del agg\n",
    "#     gc.collect()\n",
    "        \n",
    "#     # メモリ削減\n",
    "#     with t.timer(f'reduce Mem'):\n",
    "#         X_tra = reduce_mem_usage(X_tra)\n",
    "#         X_val = reduce_mem_usage(X_val)\n",
    "#         X_tra_wo_lec = reduce_mem_usage(X_tra_wo_lec)\n",
    "#         X_val_wo_lec = reduce_mem_usage(X_val_wo_lec)\n",
    "\n",
    "#     with t.timer(f'init FE Model'):\n",
    "#         riiidFE = riiidFE(FE_MODEL_PARAMS)\n",
    "#         riiidFE.set_use_features(use_features)\n",
    "#         riiidFE.set_train_mn(X_tra_wo_lec)\n",
    "#         riiidFE.set_repeat_mn(rp_table)\n",
    "#         riiidFE.set_cat_te_dict(X_tra_wo_lec, question)   # target_encordingなどようの辞書を準備\n",
    "\n",
    "#     with t.timer(f'FE - content_id features'):\n",
    "#         riiidFE.extract_content_id_feat(\n",
    "#             X_tra_wo_lec, rp_table, et_table, question,\n",
    "#             w2v_features, g_features, corr_g_features, ge_dw_features\n",
    "#         )\n",
    "#         X_tra_wo_lec = pd.merge(X_tra_wo_lec, riiidFE.content_id_df, on='content_id', how='left')\n",
    "#         X_val_wo_lec = pd.merge(X_val_wo_lec, riiidFE.content_id_df, on='content_id', how='left')\n",
    "\n",
    "    # with t.timer(f'FE - content_id with features'):\n",
    "    #     riiidFE.extract_content_idxxxprior_question_had_explanation_feat(X_tra_wo_lec, rp_table, et_table)\n",
    "    #     X_tra_wo_lec = pd.merge(X_tra_wo_lec, riiidFE.content_idxxxprior_question_had_explanation_df, on=['content_id', 'prior_question_had_explanation'], how='left')\n",
    "    #     X_val_wo_lec = pd.merge(X_val_wo_lec, riiidFE.content_idxxxprior_question_had_explanation_df, on=['content_id', 'prior_question_had_explanation'], how='left')\n",
    "\n",
    "#     del w2v_features\n",
    "#     del g_features\n",
    "#     del corr_g_features\n",
    "#     del ge_dw_features\n",
    "#     del rp_table\n",
    "#     del et_table\n",
    "#     gc.collect()\n",
    "\n",
    "#     with t.timer(f'reduce Mem'):\n",
    "#         X_tra_wo_lec = reduce_mem_usage(X_tra_wo_lec)\n",
    "#         X_val_wo_lec = reduce_mem_usage(X_val_wo_lec)\n",
    "\n",
    "#     with t.timer(f'FE - loop features'):\n",
    "\n",
    "#         user_feat_df = riiidFE.add_user_feats(X_tra, add_feat=True, update_dict=True, val=False)\n",
    "#         del X_tra\n",
    "#         gc.collect()\n",
    "\n",
    "#         X_tra_wo_lec = pd.concat([X_tra_wo_lec, user_feat_df], axis=1)\n",
    "\n",
    "#         del user_feat_df\n",
    "#         gc.collect()\n",
    "#         X_tra_wo_lec = reduce_mem_usage(X_tra_wo_lec)\n",
    "\n",
    "#         # add tra only feaures\n",
    "#         smooth = 10\n",
    "#         tar = 'answered_correctly'\n",
    "#         col_list = [\n",
    "#             ['prev_part_s1', 'part'],\n",
    "#             ['prev_part_s2', 'prev_part_s1', 'part'],\n",
    "#             ['prev_part_s3', 'prev_part_s2', 'prev_part_s1', 'part'],\n",
    "#             # ['prev_question_id_s1', 'question_id'],\n",
    "#             # ['prev_question_id_s2', 'prev_question_id_s1', 'question_id'],\n",
    "#         ]\n",
    "#         for col in col_list:\n",
    "#             tra_te_dict = pd.read_feather(f'../save/features_{FOLD_NAME}/{col[-1]}_sequence_s{len(col)-1}_tra_te_sm{smooth}.feather')\n",
    "#             X_tra_wo_lec = pd.merge(X_tra_wo_lec, tra_te_dict, on=['row_id'], how='left')\n",
    "\n",
    "#         # val only flag\n",
    "#         user_feat_df = riiidFE.add_user_feats(X_val, add_feat=True, update_dict=True, val=True)\n",
    "#         del X_val\n",
    "#         gc.collect()\n",
    "\n",
    "#         X_val_wo_lec = pd.concat([X_val_wo_lec, user_feat_df], axis=1)\n",
    "\n",
    "#         del user_feat_df\n",
    "#         gc.collect()\n",
    "#         X_val_wo_lec = reduce_mem_usage(X_val_wo_lec)\n",
    "\n",
    "#         # test\n",
    "#         # riiidFE.add_user_feats(X_val, add_feat=False, update_dict=True)\n",
    "#         # riiidFE.add_user_feats(X_val, add_feat=True, update_dict=False)\n",
    "#         # print(fdsa)\n",
    "\n",
    "#     if SAVE_EXTACTED_FEATURES is True:\n",
    "#         X_tra_wo_lec.to_feather(f'../save/{EXP_NAME}_{run_id}/X_tra_wo_lec.feather')\n",
    "#         X_val_wo_lec.to_feather(f'../save/{EXP_NAME}_{run_id}/X_val_wo_lec.feather')\n",
    "\n",
    "# #     with t.timer(f'Save Model'):\n",
    "# #         os.mkdir(f'../save/{EXP_NAME}_{run_id}/model')\n",
    "# #         save_as_pkl(riiidFE, f'../save/{EXP_NAME}_{run_id}/features/riiidFE.pkl')\n",
    "\n",
    "#     with t.timer(f'Train Model'):\n",
    "\n",
    "#         model = catboost.CatBoostClassifier(\n",
    "#             **CAT_PARAMS\n",
    "#         )\n",
    "#         logging.info(f'Sample Num: {len(X_tra_wo_lec)}')\n",
    "#         logging.info(f'Feature Num: {len(use_features)}')\n",
    "\n",
    "#         model.fit(\n",
    "#             X_tra_wo_lec[use_features], X_tra_wo_lec['answered_correctly'].values,\n",
    "#             cat_features=[],\n",
    "#             # cat_features=categorical_features_index,\n",
    "#             eval_set=(X_val_wo_lec[use_features], X_val_wo_lec['answered_correctly'].values),\n",
    "#             use_best_model=True,\n",
    "#             verbose=100\n",
    "#         )\n",
    "\n",
    "#         preds_val = model.predict_proba(X_val_wo_lec[use_features])[:, 1]\n",
    "#         score = roc_auc_score(X_val_wo_lec['answered_correctly'].values, preds_val)\n",
    "\n",
    "#     with t.timer(f'Feature importance + a'):\n",
    "#         imp = model.get_feature_importance()\n",
    "#         imp = pd.DataFrame(imp, index=use_features, columns=['importance']).sort_values('importance', ascending=False)\n",
    "\n",
    "#         imp = imp.reset_index()\n",
    "#         imp.columns = ['feat', 'importance']\n",
    "#         f_unique = X_tra_wo_lec[use_features].nunique().reset_index()\n",
    "#         f_unique.columns = ['feat', 'nunique']\n",
    "\n",
    "#         imp = pd.merge(imp, f_unique, on='feat')\n",
    "#         imp['min'] = X_tra_wo_lec[imp.feat.tolist()].min().values\n",
    "#         imp['max'] = X_tra_wo_lec[imp.feat.tolist()].max().values\n",
    "#         imp['mean'] = X_tra_wo_lec[imp.feat.tolist()].mean().values\n",
    "\n",
    "# #     with t.timer(f'Save Model'):\n",
    "# #         save_as_pkl(model, f'../save/{EXP_NAME}_{run_id}/model/cat_model.pkl')\n",
    "# #         imp.to_csv(f'../save/{EXP_NAME}_{run_id}/importance_{run_id}.csv')\n",
    "# #         save_mlflow(run_id, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids",
   "language": "python",
   "name": "miniconda3-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
